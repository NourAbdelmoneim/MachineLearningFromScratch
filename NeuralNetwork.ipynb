{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification with Neural Network\n",
    "\n",
    "#### This notebook implements a neural network from scratch to classify hadnwritten digits from the MNIST database."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits # The MNIST data set is in scikit learn data set\n",
    "from sklearn.preprocessing import StandardScaler  # It is important in neural networks to scale the date\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split  # The standard - train/test to prevent overfitting and choose hyperparameters\n",
    "from sklearn.metrics import accuracy_score # \n",
    "import numpy as np\n",
    "import numpy.random as r\n",
    "import random\n",
    "import matplotlib.pyplot as plt \n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and Preprocessing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "digits = load_digits()\n",
    "\n",
    "X = digits.data\n",
    "y = digits.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the digits dataset:\n",
      "(1797, 64)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\n",
      "[ 0.  0.  5. 13.  9.  1.  0.  0.  0.  0. 13. 15. 10. 15.  5.  0.  0.  3.\n",
      " 15.  2.  0. 11.  8.  0.  0.  4. 12.  0.  0.  8.  8.  0.  0.  5.  8.  0.\n",
      "  0.  9.  8.  0.  0.  4. 11.  0.  1. 12.  7.  0.  0.  2. 14.  5. 10. 12.\n",
      "  0.  0.  0.  0.  6. 13. 10.  0.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "print(\"The shape of the digits dataset:\") \n",
    "print(X.shape)\n",
    "\n",
    "\n",
    "plt.gray()\n",
    "\n",
    "plt.matshow(digits.images[0])\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(y[0:1])\n",
    "print(X[0,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling Data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using minMaxScaler to scale data set from -1 to 1 for the tanh activation function and from 0 to 1 for the sigmoid and ReLu functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "scalerTan = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "scalerTan = scalerTan.fit(X)\n",
    "\n",
    "X = scalerTan.transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating training and testing sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting data set into training and testing sets with 60% of the data used for training and 40% used for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.6)\n",
    "    \n",
    "y_train = y_train.reshape((len(y_train)),1)\n",
    "y_test = y_test.reshape((len(y_test)),1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up the output layer "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "convert_y_to_vect( ) is a function that takes the labels of the data set and converts them to vectors using one-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def convert_y_to_vect(y):\n",
    "    target = np.zeros((10,1))\n",
    "    target[y] = 1.0\n",
    "    return target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reshaping training and testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train = [np.reshape(x, (64, 1)) for x in X_train]\n",
    "\n",
    "Y_train = []\n",
    "\n",
    "for y in y_train:\n",
    "    Y_train.append(convert_y_to_vect(y))\n",
    "    \n",
    "trainingSet = zip(X_train, Y_train)\n",
    "   \n",
    "X_test = [np.reshape(x, (64, 1)) for x in X_test]\n",
    "\n",
    "testSet = zip(X_test, y_test)\n",
    "\n",
    "trainingSet = list(trainingSet)\n",
    "testSet = list(testSet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation Functions\n",
    "f(z) implements the sigmoid, tanh, and ReLu activation functions (un-comment the desired function)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def f(z):\n",
    "    ## SIGMOID\n",
    "    #return 1.0/(1.0+np.exp(-z))\n",
    "    \n",
    "    ## TANH\n",
    "    #return (np.exp(z)-np.exp(-z))/(np.exp(z)+np.exp(-z))\n",
    "    \n",
    "    ## ReLu  \n",
    "    r = np.empty((len(z),1))\n",
    "    \n",
    "    for i in range(len(z)):\n",
    "        r[i] = np.maximum(0,z[i])\n",
    "    return r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "f_deriv(z) implements the derivative of each of the activation functions (sigmoid, tanh, and ReLu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def f_deriv(z):\n",
    "    ## SIGMOID\n",
    "    #return f(z)*(1-f(z))\n",
    "\n",
    "    ## TANH\n",
    "    #return 1 - np.power(f(z), 2)\n",
    "    \n",
    "    ##ReLu\n",
    "    r = np.empty((len(z),1))\n",
    "    \n",
    "    for i in range(len(z)):\n",
    "        if z[i]<=0:\n",
    "            r[i] = 0\n",
    "        else:\n",
    "            r[i] = 1\n",
    "    return r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing Weights and Bias\n",
    "\n",
    "setup_and_init_weights( ) is a function that takes the structure of the neural network as input and initializes the weights and biases from a random normal distriburion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def setup_and_init_weights(nn_structure):\n",
    "    \n",
    "    weights = []\n",
    "    bias = []\n",
    "    \n",
    "    for l in range(len(nn_structure)-1):\n",
    "        weights.append(np.random.standard_normal(size = (nn_structure[l+1],nn_structure[l])))\n",
    "        bias.append(np.random.standard_normal(size = (nn_structure[l+1],1)))\n",
    "\n",
    "    return weights, bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing $\\triangledown W$ and $\\triangledown b$\n",
    "\n",
    "init_tri_values( ) takes the initialized weights and biases as inputs and initializes $\\triangledown W$ and $\\triangledown b$ to arrays of 0's with the same size as the weights and biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def init_tri_values(weights, bias):\n",
    "    \n",
    "    dlt_W = []\n",
    "    dlt_b = []\n",
    "    \n",
    "    for weight in weights:\n",
    "        wD = np.zeros(weight.shape)\n",
    "        dlt_W.append(wD)\n",
    "        \n",
    "    for b in bias:\n",
    "        bD = np.zeros(b.shape)\n",
    "        dlt_b.append(bD)\n",
    "        \n",
    "    return dlt_W, dlt_b       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feed Forward \n",
    "\n",
    "feed_forward( ) takes the weights, biases, and an instancve of the data set (X) and performs a forward pass through the network returning the activations and z-vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def feed_forward(weights, bias, X):\n",
    "    \n",
    "    a1 = X\n",
    "    a = [X]\n",
    "    \n",
    "    z = []\n",
    "     \n",
    "    for i in range(len(weights)):\n",
    "        z.append(np.dot(weights[i],a1) + bias[i])\n",
    "        a1 = f(z[i])\n",
    "        a.append(a1)\n",
    "    \n",
    "    return a, z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Back Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def backPropagation(weights, bias, X, y, nn_structure):\n",
    "    \n",
    "    # initializing weight and biases\n",
    "    dlt_W, dlt_b = init_tri_values(weights, bias)\n",
    "    \n",
    "    # performing a forward pass\n",
    "    a, z_vectors = feed_forward(weights, bias, X) \n",
    "        \n",
    "    # performing a backward pass through the network using negative indices to iterate backwards\n",
    "    for nl in range(1,len(nn_structure)):  \n",
    "        \n",
    "        # calculating values for output layer\n",
    "        if nl == 1:\n",
    "            deriv = (a[-1] - y) * f_deriv(z_vectors[-1])\n",
    "            dlt_b[-1] = deriv\n",
    "            dlt_W[-1] = np.dot(deriv, a[-2].transpose())\n",
    "        \n",
    "        # calculating values for other layers\n",
    "        else:\n",
    "            z = z_vectors[-nl]\n",
    "            deriv = np.dot(weights[-nl+1].transpose(), deriv) * f_deriv(z)\n",
    "            dlt_b[-nl] = deriv\n",
    "            dlt_W[-nl] = np.dot(deriv, a[-nl-1].transpose())\n",
    "        \n",
    "    return dlt_b, dlt_W  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Evaluating Neural Network\n",
    "\n",
    "train_nn( ) takes the data set and initialized weights/ biases along with the neural network structure and performs stochastic gradient descent to train the neural network. It also takes the number of epochs, the batch size, and the learning rate to be used for the gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def train_nn(trainingSet, testSet, weights, bias, nn_structure, nBatches, batch_size, learning_rate):\n",
    "    \n",
    "    # storing number of data points\n",
    "    nData = len(trainingSet)\n",
    "    \n",
    "    # gradient descent \n",
    "    for i in range(nBatches):\n",
    "        \n",
    "        # shuffling the data set to pick batch\n",
    "        random.shuffle(trainingSet)\n",
    "        batches = []\n",
    "        \n",
    "        # getting a batches of size batch_size from the data set\n",
    "        for n in range(0,nData,batch_size):\n",
    "            batches.append(trainingSet[n:n+batch_size])\n",
    "        \n",
    "        # iterating through the batches\n",
    "        for batch in batches:\n",
    "            \n",
    "            # initializing dlt_W and dlt_b\n",
    "            dlt_W,dlt_b = init_tri_values(weights,bias)\n",
    "            \n",
    "            # performing back propagation and updating dlt_W and dlt_b accordingly \n",
    "            for X,y in batch:\n",
    "                b, w = backPropagation(weights, bias, X,y, nn_structure)\n",
    "                for i in range(len(dlt_W)):\n",
    "                    dlt_W[i] = dlt_W[i]+w[i]\n",
    "                for i in range(len(dlt_b)):\n",
    "                    dlt_b[i] = dlt_b[i]+b[i]\n",
    "            \n",
    "            # updating weights using dlt_W and the learning rate\n",
    "            for i in range(len(weights)):\n",
    "                weights[i] = weights[i]-learning_rate*dlt_W[i]\n",
    "\n",
    "            # updating biases using dlt_b and the learning rate\n",
    "            for i in range(len(bias)):\n",
    "                bias[i] = bias[i]-learning_rate*dlt_b[i]\n",
    "        \n",
    "        # making predictions and reporting accuracy result\n",
    "        nCorrect = predict(testSet,weights,bias)\n",
    "        print(\"Accuracy: \", nCorrect, \"/\", len(testSet))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making Predictions\n",
    "\n",
    "The function predict( ) takes the test set and the trained weights and biases as inputs. It performs a forward pass through the network using the weights and biases and compares the predicted result to the test label. It then returns the number of correct predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def predict(testSet,weights,bias):\n",
    "    \n",
    "    # initializing number of correct predictions to 0\n",
    "    correct = 0\n",
    "    \n",
    "    # for each data point in the test set, predicts a label then compares it to test label\n",
    "    for x, y in testSet:\n",
    "        \n",
    "        # making prediction (forward pass)\n",
    "        for i in range(len(weights)):\n",
    "            x = f(np.dot(weights[i],x)+bias[i])\n",
    "        \n",
    "        # maximum value of output of forward pass is the prediction\n",
    "        result = np.argmax(x)\n",
    "        \n",
    "        # comparing prediction to test label\n",
    "        if result == y:\n",
    "            correct+=1\n",
    "    \n",
    "    # returning the number of correct predictions\n",
    "    return correct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Network and Displaying Results "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sigmoid Function\n",
    "\n",
    "With a structure of  [64, 30, 10], the best accuracy for the neural network was 703/719 (97.77%) with a learning rate of 0.4. Changing the parameters to  [64, 10 , 10] yielded slightly lower accuracy with 682/719 (94.85%) at its peak (the learning rate was adjusted for experimentation and resulted in better accuracy with a lower learning rate (0.1)). Changing the parameters to  [64, 50, 10] resulted in a similar accuracy to the first one (702/719 or 97.64%). Increasing the second layer to 100 resulted in a significantly lower accuracy of 420/719 (58.41%). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "weights, bias = setup_and_init_weights([64,30,10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  493 / 719\n",
      "Accuracy:  587 / 719\n",
      "Accuracy:  659 / 719\n",
      "Accuracy:  661 / 719\n",
      "Accuracy:  674 / 719\n",
      "Accuracy:  686 / 719\n",
      "Accuracy:  683 / 719\n",
      "Accuracy:  692 / 719\n",
      "Accuracy:  687 / 719\n",
      "Accuracy:  688 / 719\n",
      "Accuracy:  691 / 719\n",
      "Accuracy:  694 / 719\n",
      "Accuracy:  695 / 719\n",
      "Accuracy:  689 / 719\n",
      "Accuracy:  695 / 719\n",
      "Accuracy:  693 / 719\n",
      "Accuracy:  699 / 719\n",
      "Accuracy:  696 / 719\n",
      "Accuracy:  699 / 719\n",
      "Accuracy:  696 / 719\n",
      "Accuracy:  696 / 719\n",
      "Accuracy:  695 / 719\n",
      "Accuracy:  699 / 719\n",
      "Accuracy:  701 / 719\n",
      "Accuracy:  699 / 719\n",
      "Accuracy:  700 / 719\n",
      "Accuracy:  699 / 719\n",
      "Accuracy:  696 / 719\n",
      "Accuracy:  702 / 719\n",
      "Accuracy:  701 / 719\n",
      "Accuracy:  701 / 719\n",
      "Accuracy:  701 / 719\n",
      "Accuracy:  703 / 719\n",
      "Accuracy:  700 / 719\n",
      "Accuracy:  701 / 719\n",
      "Accuracy:  701 / 719\n",
      "Accuracy:  702 / 719\n",
      "Accuracy:  701 / 719\n",
      "Accuracy:  703 / 719\n",
      "Accuracy:  703 / 719\n",
      "Accuracy:  702 / 719\n",
      "Accuracy:  701 / 719\n",
      "Accuracy:  701 / 719\n",
      "Accuracy:  701 / 719\n",
      "Accuracy:  699 / 719\n",
      "Accuracy:  701 / 719\n",
      "Accuracy:  701 / 719\n",
      "Accuracy:  702 / 719\n",
      "Accuracy:  700 / 719\n",
      "Accuracy:  701 / 719\n"
     ]
    }
   ],
   "source": [
    "train_nn(trainingSet, testSet, weights, bias,[64,30,10],50, 10, 0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "weights, bias = setup_and_init_weights([64,10,10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  224 / 719\n",
      "Accuracy:  304 / 719\n",
      "Accuracy:  378 / 719\n",
      "Accuracy:  446 / 719\n",
      "Accuracy:  469 / 719\n",
      "Accuracy:  510 / 719\n",
      "Accuracy:  521 / 719\n",
      "Accuracy:  531 / 719\n",
      "Accuracy:  556 / 719\n",
      "Accuracy:  570 / 719\n",
      "Accuracy:  603 / 719\n",
      "Accuracy:  602 / 719\n",
      "Accuracy:  627 / 719\n",
      "Accuracy:  644 / 719\n",
      "Accuracy:  640 / 719\n",
      "Accuracy:  645 / 719\n",
      "Accuracy:  653 / 719\n",
      "Accuracy:  653 / 719\n",
      "Accuracy:  657 / 719\n",
      "Accuracy:  653 / 719\n",
      "Accuracy:  661 / 719\n",
      "Accuracy:  660 / 719\n",
      "Accuracy:  670 / 719\n",
      "Accuracy:  663 / 719\n",
      "Accuracy:  666 / 719\n",
      "Accuracy:  674 / 719\n",
      "Accuracy:  666 / 719\n",
      "Accuracy:  673 / 719\n",
      "Accuracy:  673 / 719\n",
      "Accuracy:  668 / 719\n",
      "Accuracy:  675 / 719\n",
      "Accuracy:  673 / 719\n",
      "Accuracy:  674 / 719\n",
      "Accuracy:  677 / 719\n",
      "Accuracy:  674 / 719\n",
      "Accuracy:  680 / 719\n",
      "Accuracy:  681 / 719\n",
      "Accuracy:  680 / 719\n",
      "Accuracy:  679 / 719\n",
      "Accuracy:  679 / 719\n",
      "Accuracy:  679 / 719\n",
      "Accuracy:  681 / 719\n",
      "Accuracy:  681 / 719\n",
      "Accuracy:  680 / 719\n",
      "Accuracy:  679 / 719\n",
      "Accuracy:  682 / 719\n",
      "Accuracy:  680 / 719\n",
      "Accuracy:  676 / 719\n",
      "Accuracy:  682 / 719\n",
      "Accuracy:  682 / 719\n"
     ]
    }
   ],
   "source": [
    "train_nn(trainingSet, testSet, weights, bias,[64,10,10],50, 10, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "weights, bias = setup_and_init_weights([64,50,10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  347 / 719\n",
      "Accuracy:  557 / 719\n",
      "Accuracy:  618 / 719\n",
      "Accuracy:  655 / 719\n",
      "Accuracy:  683 / 719\n",
      "Accuracy:  682 / 719\n",
      "Accuracy:  688 / 719\n",
      "Accuracy:  691 / 719\n",
      "Accuracy:  695 / 719\n",
      "Accuracy:  685 / 719\n",
      "Accuracy:  691 / 719\n",
      "Accuracy:  694 / 719\n",
      "Accuracy:  697 / 719\n",
      "Accuracy:  694 / 719\n",
      "Accuracy:  696 / 719\n",
      "Accuracy:  692 / 719\n",
      "Accuracy:  697 / 719\n",
      "Accuracy:  699 / 719\n",
      "Accuracy:  695 / 719\n",
      "Accuracy:  699 / 719\n",
      "Accuracy:  700 / 719\n",
      "Accuracy:  698 / 719\n",
      "Accuracy:  696 / 719\n",
      "Accuracy:  698 / 719\n",
      "Accuracy:  698 / 719\n",
      "Accuracy:  699 / 719\n",
      "Accuracy:  698 / 719\n",
      "Accuracy:  697 / 719\n",
      "Accuracy:  697 / 719\n",
      "Accuracy:  700 / 719\n",
      "Accuracy:  700 / 719\n",
      "Accuracy:  697 / 719\n",
      "Accuracy:  699 / 719\n",
      "Accuracy:  698 / 719\n",
      "Accuracy:  699 / 719\n",
      "Accuracy:  697 / 719\n",
      "Accuracy:  699 / 719\n",
      "Accuracy:  696 / 719\n",
      "Accuracy:  699 / 719\n",
      "Accuracy:  698 / 719\n",
      "Accuracy:  697 / 719\n",
      "Accuracy:  698 / 719\n",
      "Accuracy:  699 / 719\n",
      "Accuracy:  698 / 719\n",
      "Accuracy:  699 / 719\n",
      "Accuracy:  702 / 719\n",
      "Accuracy:  699 / 719\n",
      "Accuracy:  701 / 719\n",
      "Accuracy:  699 / 719\n",
      "Accuracy:  699 / 719\n"
     ]
    }
   ],
   "source": [
    "train_nn(trainingSet, testSet, weights, bias,[64,50,10],50, 10, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "weights, bias = setup_and_init_weights([64,100,10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  137 / 719\n",
      "Accuracy:  224 / 719\n",
      "Accuracy:  290 / 719\n",
      "Accuracy:  336 / 719\n",
      "Accuracy:  354 / 719\n",
      "Accuracy:  360 / 719\n",
      "Accuracy:  362 / 719\n",
      "Accuracy:  371 / 719\n",
      "Accuracy:  406 / 719\n",
      "Accuracy:  414 / 719\n",
      "Accuracy:  416 / 719\n",
      "Accuracy:  413 / 719\n",
      "Accuracy:  415 / 719\n",
      "Accuracy:  415 / 719\n",
      "Accuracy:  415 / 719\n",
      "Accuracy:  416 / 719\n",
      "Accuracy:  411 / 719\n",
      "Accuracy:  413 / 719\n",
      "Accuracy:  409 / 719\n",
      "Accuracy:  412 / 719\n",
      "Accuracy:  413 / 719\n",
      "Accuracy:  416 / 719\n",
      "Accuracy:  414 / 719\n",
      "Accuracy:  414 / 719\n",
      "Accuracy:  414 / 719\n",
      "Accuracy:  412 / 719\n",
      "Accuracy:  416 / 719\n",
      "Accuracy:  416 / 719\n",
      "Accuracy:  413 / 719\n",
      "Accuracy:  416 / 719\n",
      "Accuracy:  414 / 719\n",
      "Accuracy:  414 / 719\n",
      "Accuracy:  416 / 719\n",
      "Accuracy:  416 / 719\n",
      "Accuracy:  415 / 719\n",
      "Accuracy:  415 / 719\n",
      "Accuracy:  416 / 719\n",
      "Accuracy:  414 / 719\n",
      "Accuracy:  415 / 719\n",
      "Accuracy:  414 / 719\n",
      "Accuracy:  415 / 719\n",
      "Accuracy:  415 / 719\n",
      "Accuracy:  416 / 719\n",
      "Accuracy:  416 / 719\n",
      "Accuracy:  415 / 719\n",
      "Accuracy:  416 / 719\n",
      "Accuracy:  418 / 719\n",
      "Accuracy:  418 / 719\n",
      "Accuracy:  420 / 719\n",
      "Accuracy:  419 / 719\n"
     ]
    }
   ],
   "source": [
    "train_nn(trainingSet, testSet, weights, bias,[64,100,10],50, 10, 0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TanH Function\n",
    "\n",
    "With a structure of [64,30,10] the TanH activation function performed best with a learning rate of 0.05 and got an accuracy of 608/719 (84.56%). With [64,50,10] it performed slightly worse with an accuracy of 583/719 (81.08%). With [64,10,10], it performed worse with 476/719 (66.2%), which also required a higher learning rate of 0.8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  137 / 719\n",
      "Accuracy:  200 / 719\n",
      "Accuracy:  121 / 719\n",
      "Accuracy:  297 / 719\n",
      "Accuracy:  252 / 719\n",
      "Accuracy:  323 / 719\n",
      "Accuracy:  203 / 719\n",
      "Accuracy:  449 / 719\n",
      "Accuracy:  345 / 719\n",
      "Accuracy:  400 / 719\n",
      "Accuracy:  208 / 719\n",
      "Accuracy:  518 / 719\n",
      "Accuracy:  414 / 719\n",
      "Accuracy:  459 / 719\n",
      "Accuracy:  297 / 719\n",
      "Accuracy:  560 / 719\n",
      "Accuracy:  442 / 719\n",
      "Accuracy:  423 / 719\n",
      "Accuracy:  482 / 719\n",
      "Accuracy:  427 / 719\n",
      "Accuracy:  331 / 719\n",
      "Accuracy:  425 / 719\n",
      "Accuracy:  435 / 719\n",
      "Accuracy:  396 / 719\n",
      "Accuracy:  423 / 719\n",
      "Accuracy:  145 / 719\n",
      "Accuracy:  440 / 719\n",
      "Accuracy:  545 / 719\n",
      "Accuracy:  518 / 719\n",
      "Accuracy:  448 / 719\n",
      "Accuracy:  268 / 719\n",
      "Accuracy:  345 / 719\n",
      "Accuracy:  327 / 719\n",
      "Accuracy:  507 / 719\n",
      "Accuracy:  435 / 719\n",
      "Accuracy:  332 / 719\n",
      "Accuracy:  379 / 719\n",
      "Accuracy:  517 / 719\n",
      "Accuracy:  480 / 719\n",
      "Accuracy:  421 / 719\n",
      "Accuracy:  408 / 719\n",
      "Accuracy:  434 / 719\n",
      "Accuracy:  510 / 719\n",
      "Accuracy:  373 / 719\n",
      "Accuracy:  559 / 719\n",
      "Accuracy:  551 / 719\n",
      "Accuracy:  577 / 719\n",
      "Accuracy:  528 / 719\n",
      "Accuracy:  608 / 719\n",
      "Accuracy:  585 / 719\n"
     ]
    }
   ],
   "source": [
    "weights, bias = setup_and_init_weights([64,30,10])\n",
    "train_nn(trainingSet, testSet, weights, bias,[64,30,10],50, 10, 0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  145 / 719\n",
      "Accuracy:  319 / 719\n",
      "Accuracy:  322 / 719\n",
      "Accuracy:  319 / 719\n",
      "Accuracy:  106 / 719\n",
      "Accuracy:  232 / 719\n",
      "Accuracy:  163 / 719\n",
      "Accuracy:  160 / 719\n",
      "Accuracy:  233 / 719\n",
      "Accuracy:  230 / 719\n",
      "Accuracy:  197 / 719\n",
      "Accuracy:  374 / 719\n",
      "Accuracy:  199 / 719\n",
      "Accuracy:  391 / 719\n",
      "Accuracy:  223 / 719\n",
      "Accuracy:  292 / 719\n",
      "Accuracy:  470 / 719\n",
      "Accuracy:  417 / 719\n",
      "Accuracy:  411 / 719\n",
      "Accuracy:  417 / 719\n",
      "Accuracy:  382 / 719\n",
      "Accuracy:  577 / 719\n",
      "Accuracy:  456 / 719\n",
      "Accuracy:  514 / 719\n",
      "Accuracy:  494 / 719\n",
      "Accuracy:  388 / 719\n",
      "Accuracy:  416 / 719\n",
      "Accuracy:  391 / 719\n",
      "Accuracy:  231 / 719\n",
      "Accuracy:  427 / 719\n",
      "Accuracy:  474 / 719\n",
      "Accuracy:  201 / 719\n",
      "Accuracy:  258 / 719\n",
      "Accuracy:  454 / 719\n",
      "Accuracy:  419 / 719\n",
      "Accuracy:  215 / 719\n",
      "Accuracy:  224 / 719\n",
      "Accuracy:  497 / 719\n",
      "Accuracy:  230 / 719\n",
      "Accuracy:  408 / 719\n",
      "Accuracy:  270 / 719\n",
      "Accuracy:  555 / 719\n",
      "Accuracy:  171 / 719\n",
      "Accuracy:  415 / 719\n",
      "Accuracy:  487 / 719\n",
      "Accuracy:  257 / 719\n",
      "Accuracy:  583 / 719\n",
      "Accuracy:  561 / 719\n",
      "Accuracy:  434 / 719\n",
      "Accuracy:  410 / 719\n"
     ]
    }
   ],
   "source": [
    "weights, bias = setup_and_init_weights([64,50,10])\n",
    "train_nn(trainingSet, testSet, weights, bias,[64,50,10],50, 10, 0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  213 / 719\n",
      "Accuracy:  92 / 719\n",
      "Accuracy:  180 / 719\n",
      "Accuracy:  223 / 719\n",
      "Accuracy:  294 / 719\n",
      "Accuracy:  161 / 719\n",
      "Accuracy:  349 / 719\n",
      "Accuracy:  257 / 719\n",
      "Accuracy:  183 / 719\n",
      "Accuracy:  335 / 719\n",
      "Accuracy:  419 / 719\n",
      "Accuracy:  234 / 719\n",
      "Accuracy:  328 / 719\n",
      "Accuracy:  205 / 719\n",
      "Accuracy:  280 / 719\n",
      "Accuracy:  322 / 719\n",
      "Accuracy:  293 / 719\n",
      "Accuracy:  428 / 719\n",
      "Accuracy:  342 / 719\n",
      "Accuracy:  340 / 719\n",
      "Accuracy:  123 / 719\n",
      "Accuracy:  316 / 719\n",
      "Accuracy:  276 / 719\n",
      "Accuracy:  311 / 719\n",
      "Accuracy:  389 / 719\n",
      "Accuracy:  273 / 719\n",
      "Accuracy:  202 / 719\n",
      "Accuracy:  389 / 719\n",
      "Accuracy:  374 / 719\n",
      "Accuracy:  263 / 719\n",
      "Accuracy:  411 / 719\n",
      "Accuracy:  521 / 719\n",
      "Accuracy:  295 / 719\n",
      "Accuracy:  314 / 719\n",
      "Accuracy:  288 / 719\n",
      "Accuracy:  476 / 719\n",
      "Accuracy:  449 / 719\n",
      "Accuracy:  192 / 719\n",
      "Accuracy:  411 / 719\n",
      "Accuracy:  428 / 719\n",
      "Accuracy:  278 / 719\n",
      "Accuracy:  369 / 719\n",
      "Accuracy:  300 / 719\n",
      "Accuracy:  265 / 719\n",
      "Accuracy:  247 / 719\n",
      "Accuracy:  377 / 719\n",
      "Accuracy:  364 / 719\n",
      "Accuracy:  380 / 719\n",
      "Accuracy:  435 / 719\n",
      "Accuracy:  319 / 719\n"
     ]
    }
   ],
   "source": [
    "weights, bias = setup_and_init_weights([64,10,10])\n",
    "train_nn(trainingSet, testSet, weights, bias,[64,10,10],50, 10, 0.08)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ReLu Function\n",
    "\n",
    "The ReLu function did not perform very well for this task. For a structure of [64,30,10] it had an accuracy of 82/719 (11.4%). With [64,50,10], and a learning rate of 0.00001, it got an accuracy of 76/719 (10.57%). With [64,10,10] it performed better with 163/719 (22.67%) also with a learning rate of 0.00001. Smaller learning rates seemed to work better with the ReLu function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  44 / 719\n",
      "Accuracy:  70 / 719\n",
      "Accuracy:  73 / 719\n",
      "Accuracy:  77 / 719\n",
      "Accuracy:  78 / 719\n",
      "Accuracy:  76 / 719\n",
      "Accuracy:  82 / 719\n",
      "Accuracy:  81 / 719\n",
      "Accuracy:  80 / 719\n",
      "Accuracy:  80 / 719\n",
      "Accuracy:  79 / 719\n",
      "Accuracy:  78 / 719\n",
      "Accuracy:  78 / 719\n",
      "Accuracy:  78 / 719\n",
      "Accuracy:  76 / 719\n",
      "Accuracy:  76 / 719\n",
      "Accuracy:  76 / 719\n",
      "Accuracy:  76 / 719\n",
      "Accuracy:  77 / 719\n",
      "Accuracy:  77 / 719\n",
      "Accuracy:  77 / 719\n",
      "Accuracy:  77 / 719\n",
      "Accuracy:  77 / 719\n",
      "Accuracy:  77 / 719\n",
      "Accuracy:  78 / 719\n",
      "Accuracy:  78 / 719\n",
      "Accuracy:  78 / 719\n",
      "Accuracy:  78 / 719\n",
      "Accuracy:  78 / 719\n",
      "Accuracy:  78 / 719\n",
      "Accuracy:  78 / 719\n",
      "Accuracy:  78 / 719\n",
      "Accuracy:  78 / 719\n",
      "Accuracy:  78 / 719\n",
      "Accuracy:  78 / 719\n",
      "Accuracy:  77 / 719\n",
      "Accuracy:  77 / 719\n",
      "Accuracy:  77 / 719\n",
      "Accuracy:  77 / 719\n",
      "Accuracy:  77 / 719\n",
      "Accuracy:  77 / 719\n",
      "Accuracy:  77 / 719\n",
      "Accuracy:  77 / 719\n",
      "Accuracy:  76 / 719\n",
      "Accuracy:  76 / 719\n",
      "Accuracy:  76 / 719\n",
      "Accuracy:  76 / 719\n",
      "Accuracy:  76 / 719\n",
      "Accuracy:  75 / 719\n",
      "Accuracy:  75 / 719\n"
     ]
    }
   ],
   "source": [
    "weights, bias = setup_and_init_weights([64,30,10])\n",
    "train_nn(trainingSet, testSet, weights, bias,[64,30,10],50, 30, 0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  53 / 719\n",
      "Accuracy:  61 / 719\n",
      "Accuracy:  70 / 719\n",
      "Accuracy:  76 / 719\n",
      "Accuracy:  76 / 719\n",
      "Accuracy:  74 / 719\n",
      "Accuracy:  70 / 719\n",
      "Accuracy:  71 / 719\n",
      "Accuracy:  72 / 719\n",
      "Accuracy:  74 / 719\n",
      "Accuracy:  73 / 719\n",
      "Accuracy:  75 / 719\n",
      "Accuracy:  74 / 719\n",
      "Accuracy:  74 / 719\n",
      "Accuracy:  75 / 719\n",
      "Accuracy:  74 / 719\n",
      "Accuracy:  74 / 719\n",
      "Accuracy:  73 / 719\n",
      "Accuracy:  73 / 719\n",
      "Accuracy:  73 / 719\n",
      "Accuracy:  73 / 719\n",
      "Accuracy:  73 / 719\n",
      "Accuracy:  73 / 719\n",
      "Accuracy:  73 / 719\n",
      "Accuracy:  73 / 719\n",
      "Accuracy:  72 / 719\n",
      "Accuracy:  72 / 719\n",
      "Accuracy:  72 / 719\n",
      "Accuracy:  72 / 719\n",
      "Accuracy:  72 / 719\n",
      "Accuracy:  72 / 719\n",
      "Accuracy:  72 / 719\n",
      "Accuracy:  72 / 719\n",
      "Accuracy:  72 / 719\n",
      "Accuracy:  72 / 719\n",
      "Accuracy:  72 / 719\n",
      "Accuracy:  72 / 719\n",
      "Accuracy:  72 / 719\n",
      "Accuracy:  72 / 719\n",
      "Accuracy:  72 / 719\n",
      "Accuracy:  72 / 719\n",
      "Accuracy:  72 / 719\n",
      "Accuracy:  72 / 719\n",
      "Accuracy:  72 / 719\n",
      "Accuracy:  72 / 719\n",
      "Accuracy:  72 / 719\n",
      "Accuracy:  72 / 719\n",
      "Accuracy:  72 / 719\n",
      "Accuracy:  73 / 719\n",
      "Accuracy:  73 / 719\n"
     ]
    }
   ],
   "source": [
    "weights, bias = setup_and_init_weights([64,50,10])\n",
    "train_nn(trainingSet, testSet, weights, bias,[64,50,10],50, 5, 0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  105 / 719\n",
      "Accuracy:  106 / 719\n",
      "Accuracy:  102 / 719\n",
      "Accuracy:  106 / 719\n",
      "Accuracy:  111 / 719\n",
      "Accuracy:  120 / 719\n",
      "Accuracy:  124 / 719\n",
      "Accuracy:  130 / 719\n",
      "Accuracy:  140 / 719\n",
      "Accuracy:  146 / 719\n",
      "Accuracy:  146 / 719\n",
      "Accuracy:  149 / 719\n",
      "Accuracy:  151 / 719\n",
      "Accuracy:  155 / 719\n",
      "Accuracy:  158 / 719\n",
      "Accuracy:  154 / 719\n",
      "Accuracy:  156 / 719\n",
      "Accuracy:  155 / 719\n",
      "Accuracy:  159 / 719\n",
      "Accuracy:  160 / 719\n",
      "Accuracy:  163 / 719\n",
      "Accuracy:  163 / 719\n",
      "Accuracy:  162 / 719\n",
      "Accuracy:  162 / 719\n",
      "Accuracy:  162 / 719\n",
      "Accuracy:  161 / 719\n",
      "Accuracy:  159 / 719\n",
      "Accuracy:  159 / 719\n",
      "Accuracy:  157 / 719\n",
      "Accuracy:  156 / 719\n",
      "Accuracy:  155 / 719\n",
      "Accuracy:  155 / 719\n",
      "Accuracy:  154 / 719\n",
      "Accuracy:  153 / 719\n",
      "Accuracy:  152 / 719\n",
      "Accuracy:  151 / 719\n",
      "Accuracy:  149 / 719\n",
      "Accuracy:  152 / 719\n",
      "Accuracy:  151 / 719\n",
      "Accuracy:  149 / 719\n",
      "Accuracy:  149 / 719\n",
      "Accuracy:  148 / 719\n",
      "Accuracy:  147 / 719\n",
      "Accuracy:  146 / 719\n",
      "Accuracy:  146 / 719\n",
      "Accuracy:  145 / 719\n",
      "Accuracy:  145 / 719\n",
      "Accuracy:  145 / 719\n",
      "Accuracy:  145 / 719\n",
      "Accuracy:  145 / 719\n"
     ]
    }
   ],
   "source": [
    "weights, bias = setup_and_init_weights([64,10,10])\n",
    "train_nn(trainingSet, testSet, weights, bias,[64,10,10],50, 30, 0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
