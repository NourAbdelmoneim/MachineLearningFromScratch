{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import csv\n",
    "import math\n",
    "import copy\n",
    "import time\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from numpy import *\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiclass classifier decision tree using ID3 algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalize the entire dataset prior to learning using min-max normalization \n",
    "def normalize(matrix):\n",
    "#   transfer the data metrix to np array in float type.\n",
    "    X = np.asarray(matrix)\n",
    "    print(\"normalizing the entire dataset:\")\n",
    "    #print(X)\n",
    "    print(\"Before normalizing\")\n",
    "#   apply the normalization along the 0 axis of a using the formula: (x - x_min)/(x_max - x_min)\n",
    "    X = (X-np.amin(X,axis=0))/(np.amax(X,axis=0)-np.amin(X,axis=0))\n",
    "    \n",
    "    return X\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading from the file using numpy genfromtxt with delimiter ','\n",
    "def load_csv(file):\n",
    "    X = np.empty((150,4))\n",
    "    X[:,0] = np.genfromtxt(file, delimiter = \",\", dtype='float', usecols = 0)\n",
    "    X[:,1] = np.genfromtxt(file, delimiter = \",\", dtype='float', usecols = 1)\n",
    "    X[:,2] = np.genfromtxt(file, delimiter = \",\", dtype='float', usecols = 2)\n",
    "    X[:,3] = np.genfromtxt(file, delimiter = \",\", dtype='float', usecols = 3)\n",
    "    \n",
    "    y = np.empty((150,1))\n",
    "    y = np.genfromtxt(file, delimiter = \",\", dtype = \"S\", usecols = 4)\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "X,y = load_csv(\"iris.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#method to randomly shuffle the array using the numpy.random.shuffle()\n",
    "def random_numpy_array(ar):\n",
    "    np.random.shuffle(ar)\n",
    "    return ar\n",
    "\n",
    "X = random_numpy_array(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 4)\n",
      "normalizing the entire dataset:\n",
      "Before normalizing\n",
      "(150, 1)\n",
      "nTests:  15\n",
      "[[b'0.41666666666666663' b'0.2916666666666667' b'0.6949152542372881'\n",
      "  b'0.75' b'Iris-setosa']\n",
      " [b'0.5277777777777778' b'0.0833333333333334' b'0.5932203389830508'\n",
      "  b'0.5833333333333334' b'Iris-setosa']\n",
      " [b'0.5555555555555555' b'0.5416666666666665' b'0.847457627118644' b'1.0'\n",
      "  b'Iris-setosa']\n",
      " [b'0.361111111111111' b'0.2916666666666667' b'0.5423728813559322' b'0.5'\n",
      "  b'Iris-setosa']\n",
      " [b'0.30555555555555564' b'0.5833333333333333' b'0.11864406779661016'\n",
      "  b'0.04166666666666667' b'Iris-setosa']\n",
      " [b'0.5833333333333334' b'0.3333333333333332' b'0.7796610169491525'\n",
      "  b'0.8333333333333334' b'Iris-setosa']\n",
      " [b'0.44444444444444453' b'0.5' b'0.6440677966101694'\n",
      "  b'0.7083333333333334' b'Iris-setosa']\n",
      " [b'0.5833333333333334' b'0.4583333333333333' b'0.7627118644067796'\n",
      "  b'0.7083333333333334' b'Iris-setosa']\n",
      " [b'0.4722222222222222' b'0.0833333333333334' b'0.6779661016949152'\n",
      "  b'0.5833333333333334' b'Iris-setosa']\n",
      " [b'0.13888888888888887' b'0.5833333333333333' b'0.15254237288135591'\n",
      "  b'0.04166666666666667' b'Iris-setosa']\n",
      " [b'0.611111111111111' b'0.41666666666666663' b'0.711864406779661'\n",
      "  b'0.7916666666666666' b'Iris-setosa']\n",
      " [b'0.19444444444444448' b'0.5833333333333333' b'0.0847457627118644'\n",
      "  b'0.04166666666666667' b'Iris-setosa']\n",
      " [b'0.30555555555555564' b'0.5833333333333333' b'0.0847457627118644'\n",
      "  b'0.12500000000000003' b'Iris-setosa']\n",
      " [b'0.11111111111111119' b'0.5' b'0.05084745762711865'\n",
      "  b'0.04166666666666667' b'Iris-setosa']\n",
      " [b'0.7777777777777776' b'0.41666666666666663' b'0.8305084745762712'\n",
      "  b'0.8333333333333334' b'Iris-setosa']]\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)\n",
    "X = normalize(X)\n",
    "#print(X)\n",
    "\n",
    "y = y.reshape((150,1))\n",
    "print(y.shape)\n",
    "\n",
    "X = np.concatenate((X,y), axis = 1)\n",
    "\n",
    "rX = []\n",
    "#rX[0].append([i for i in range(len(y))])\n",
    "#rX[1].append([label for label in y])\n",
    "\n",
    "nTests = int(len(y)*0.1)\n",
    "\n",
    "print(\"nTests: \", nTests)\n",
    "\n",
    "startTest = 0\n",
    "\n",
    "#     set the ending index to be the number of testing data\n",
    "endTest = nTests\n",
    "\n",
    "#     create a list that store all features of the testing data\n",
    "testFeatures = X[startTest:endTest]\n",
    "\n",
    "print(testFeatures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (<ipython-input-7-f6a5ce5c010f>, line 83)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-7-f6a5ce5c010f>\"\u001b[0;36m, line \u001b[0;32m83\u001b[0m\n\u001b[0;31m    return\u001b[0m\n\u001b[0m         ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "#Normalize the data and generate the training labels,training features, test labels and test training\n",
    "def generate_set(X,y):\n",
    "    \n",
    "    print(X.shape[0])\n",
    "#     store the label X[:,-1] to Y ##already intitialized like this\n",
    "#     reshape y to (Y's length, 1)\n",
    "#     store it to j\n",
    "    \n",
    "    print(\"J is\",y)\n",
    "#     create the new_X which exclude the label X[:,:-1] ##already initialized like this\n",
    "\n",
    "#     normalize the data step\n",
    "#     using our implemented function normalize()\n",
    "    X = normalize(X)\n",
    "\n",
    "#     add the label back to the normiazlied X\n",
    "#     using np.concatenate along axis=1\n",
    "    Xy = np.concatenate(X,y, axis = 1)\n",
    "\n",
    "#     store the size of rows of the normalized X with labels\n",
    "    rX = []\n",
    "    #rX[0] = [i for i in range(y)]\n",
    "    #rX[1] = [label for label in y]\n",
    "    \n",
    "\n",
    "#     use the 10% of the data to be the test set.\n",
    "#     store the number of testing data\n",
    "    nTests = int(len(y)*0.1)\n",
    "\n",
    "#     set the starting idex to be 0\n",
    "    startTest = 0\n",
    "\n",
    "#     set the ending index to be the number of testing data\n",
    "    endTest = nTest\n",
    "\n",
    "#     create a list that store all features of the testing data\n",
    "    testFeatures = X[startTest:endTest]\n",
    "\n",
    "#     create a list that store all labels of the testing data\n",
    "    testLabels = y[startTest:endTest]\n",
    "\n",
    "#     create a list that store all features of the training data\n",
    "    trainingFeatures = X[endTest:len(X)]\n",
    "\n",
    "#     create a list that store all labels of the training data\n",
    "    trainingLabels = y[endTest:len(y)]\n",
    "\n",
    "#     10-fold cross-validation:\n",
    "    for i in range(10):\n",
    "#         store the test set for corss-validation using X[start:end,:]\n",
    "\n",
    "#         get training data before the testing data X[:start, :]\n",
    "\n",
    "#         get training data after the testing data X[:start, :]\n",
    "\n",
    "#         form the new training set using np.concatenate\n",
    "\n",
    "#         get the testing set labels\n",
    "\n",
    "#         flattent the labels\n",
    "\n",
    "#         get the training set labels\n",
    "\n",
    "#         flattent the labels\n",
    "\n",
    "#         create the test set exclude the labels\n",
    "\n",
    "#         same for the training set\n",
    "\n",
    "\n",
    "\n",
    "#         append test data of this fold to the list\n",
    "\n",
    "#         append test lables of this fold to the list\n",
    "\n",
    "#         do the same for the training set\n",
    "\n",
    "\n",
    "#         update the index pointer\n",
    "\n",
    "\n",
    "#     return the fold list that contain data and label for both training and testing set.\n",
    "    return \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build a dictionary where the key is the class label and values are the features which belong to that class.\n",
    "def build_dict_of_attributes_with_class_values(X,y):\n",
    "    \n",
    "#     init a dict for attributes\n",
    "    featureDict = dict()\n",
    "    \n",
    "#     init feature list.\n",
    "\n",
    "    features = []\n",
    "    featureIndex=[]\n",
    "\n",
    "#     for each feature in the dataset\n",
    "    for i in range(X.shape[1]):\n",
    "#         store the feature index\n",
    "        featureIndex.append(i)\n",
    "    \n",
    "#     find all the value correspond to this feature\n",
    "        values = X[:,i]\n",
    "\n",
    "#     init an attribute list\n",
    "        attribute = []\n",
    "\n",
    "#     init the counter to 0\n",
    "        c = 0\n",
    "\n",
    "#         for each value in the \"all the value correspond to this feature\"\n",
    "        for i,value in enumerate(values):\n",
    "#             init a empty list that store the attribute value\n",
    "            attributeValues = []\n",
    "\n",
    "#             append the this value to the list\n",
    "            attributeValues.append(value)\n",
    "\n",
    "#             append the label of this value to the list\n",
    "            attributeValues.append(X[i,4])\n",
    "\n",
    "#             append this list to the attribute list.\n",
    "            attribute.append(attributeValues)\n",
    "\n",
    "#             increase the counter\n",
    "            c = c+1\n",
    "\n",
    "#         add this attribute list to the dict according to the feature index\n",
    "        \n",
    "\n",
    "#         append the feature indx to the feature list.\n",
    "\n",
    "#     return the dict and feature list.\n",
    "    return \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "frequencyDict = dict()\n",
    "frequencyDict[\"key\"]= 1\n",
    "print(frequencyDict[\"key\"])\n",
    "\n",
    "frequencyDict[\"key\"] +=1\n",
    "print(frequencyDict[\"key\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterative Dichotomiser 3 entropy calculation\n",
    "def entropy(y):\n",
    "#     init a class frequence dict\n",
    "    frequencyDict = dict()\n",
    "\n",
    "#     init the attribute entropy to 0\n",
    "    entropy = 0\n",
    "\n",
    "#     for each label in y:\n",
    "    for i in y:\n",
    "#         this is label is already in the dict, we increase its feq\n",
    "        if i in frequencyDict:\n",
    "            frequencyDict[i] += 1;\n",
    "\n",
    "#          else, we set the freq to 1\n",
    "        else:\n",
    "            frequencyDict[i] = 1\n",
    "\n",
    "#     calculate the cumulate entropy using the formula.\n",
    "    p = frequencyDict.values()/len(y)\n",
    "    entropy = -sum(p*math.log2(p))\n",
    "\n",
    "    return entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Class node and explanation is self explaination\n",
    "class Node(object):\n",
    "#     init the node with val,lchild,rchild,thea and leaf.\n",
    "    def __init__(self, val, lchild, rchild,thea,leaf):\n",
    "        self.root_value = val\n",
    "        self.root_left = lchild\n",
    "        self.root_right = rchild\n",
    "        self.theta = thea\n",
    "        self.leaf = leaf\n",
    "\n",
    "#     method to identify if the node is leaf\n",
    "    def is_leaf(self):\n",
    "        if self.root_left == None and self.root_right = None:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "#     method to return threshold value\n",
    "    def ret_thetha(self):\n",
    "        return self.theta\n",
    "    \n",
    "#     method return root value\n",
    "    def ret_root_value(self):\n",
    "        return self.root_value\n",
    "    \n",
    "#     method return left tree\n",
    "    def ret_llist(self):\n",
    "        return self.root_left\n",
    "\n",
    "#     method return right tree\n",
    "    def ret_rlist(self):\n",
    "        return self.root_right\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"(%r, %r, %r, %r)\" %(self.root_value,self.root_left,self.root_right,self.theta)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Decision tree object\n",
    "class DecisionTree(object):\n",
    "#     init a variable called fea_list\n",
    "    fea_list = []\n",
    "\n",
    "#     init the Dtree by setting the root node to None\n",
    "    def __init__(self):\n",
    "        self.root_node = None\n",
    "\n",
    "    #method to return the major class value using Counter() and .most_common()\n",
    "    def cal_major_class_values(self,class_values):\n",
    "        majorClass = counter.most_common(class_values)\n",
    "        return majorClass\n",
    "\n",
    "    #method to calculate best threshold value for each feature\n",
    "    def cal_best_theta_value(self,ke,attri_list):\n",
    "#         init a list for data\n",
    "        data = []\n",
    "\n",
    "#         init a list for class labes\n",
    "        labels = []\n",
    "\n",
    "#         for each attribute in the attri_list\n",
    "        for i in attri_list:\n",
    "#             append the data\n",
    "            data.append(i)\n",
    "\n",
    "#             append the feature value. ##APPEND what??\n",
    "            labels.append(ke)\n",
    "\n",
    "#         calculate the entropy of those feaure values\n",
    "            \n",
    "\n",
    "#         init the max info gain = 0\n",
    "        maxInfoGain = 0\n",
    "\n",
    "#         init theta=0\n",
    "        theta = 0\n",
    "\n",
    "#         init a list that store the best index on the left\n",
    "        leftIndex = []\n",
    "\n",
    "#         init a list that store the best index on the right\n",
    "        rightIndex = []\n",
    "\n",
    "#         init a list that store class labels after split\n",
    "        splitLabels = []\n",
    "\n",
    "#         sort the data\n",
    "        np.sort(data)\n",
    "\n",
    "#         for each index of data:\n",
    "        for i in len(data)\n",
    "#             calculate the current theta using data[i]+data[i+1])/ 2\n",
    "            theta = data[i]+data[i+1]\n",
    "\n",
    "#             init a list that store index that less than theta\n",
    "            lessIndex = []\n",
    "\n",
    "#             init a list that store value that less than theta\n",
    "            lessValue = []\n",
    "\n",
    "#             init a list that store index that greater than theta\n",
    "            moreIndex = []\n",
    "\n",
    "#             init a list that store value that less than theta\n",
    "            moreValue = []\n",
    "\n",
    "#             init the counter to 0\n",
    "            c = 0\n",
    "\n",
    "#             for each index and value in attri_list\n",
    "            for c,j in enumerate(attri_list):\n",
    "#                 if value less or equal than the current theta:\n",
    "                if (j <= theta)\n",
    "#                     update the \"less\" list of index and value\n",
    "                    lessIndex.append(c)\n",
    "                    lessValue.append(j)\n",
    "\n",
    "#                 else:\n",
    "                else:\n",
    "#                     update the \"greater\" list of index and value\n",
    "                    moreIndex.append(c)\n",
    "                    moreValue.append(j)\n",
    "\n",
    "\n",
    "#             calculate the entropy of the \"less\" list\n",
    "            entropyLess = entropy(lessIndex) #the labels \n",
    "\n",
    "#             calculate the entropy of the \"greater\" list\n",
    "            entropyMore = entropy(moreIndex) #the labels \n",
    "\n",
    "#             calculate the info gain using the formular.\n",
    "            infoGain = entropyLess + entropyMore\n",
    "\n",
    "\n",
    "#             if current info gain > max info gan\n",
    "            if infoGain > maxInfoGain:\n",
    "#                 update the info gain, \n",
    "#                     the theta, the best index list of right, \n",
    "#                         the best index list of left and class_labels_list_after_split\n",
    "                maxInfoGain = infoGain\n",
    "                leftIndex = \n",
    "                rightIndex = \n",
    "                splitLabels = \n",
    "\n",
    "#         return the max info gain, theata,the best left list,the best right list and class label after split\n",
    "        return \n",
    "\n",
    "    #method to select the best feature out of all the features.\n",
    "    def best_feature(self,dict_rep):\n",
    "#         set key value to none\n",
    "        key = None\n",
    "\n",
    "#         set best info gain to -1\n",
    "        maxInfoGain = -1\n",
    "\n",
    "#         set best theta to 0\n",
    "        theta = 0\n",
    "\n",
    "#         set best left list to empty\n",
    "        maxLeft = []\n",
    "    \n",
    "#         set best right list to empty\n",
    "        maxRight = []\n",
    "\n",
    "#         set best class labels after split to empty\n",
    "        bestLabel = []\n",
    "\n",
    "#         init a result list\n",
    "        results = []\n",
    "\n",
    "#         for each key in dict_rep:\n",
    "        for ke in dict_rep.keys():\n",
    "#             using cal_best_theta_value() and store all returned values\n",
    "            results.append(cal_best_theta_value(ke,))\n",
    "\n",
    "#             if info gian is greater than best info gain:\n",
    "\n",
    "#                 update info gain, theth, key value,\n",
    "#                     left list, right list, class labels after split\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         append the key value to the retrun list\n",
    "\n",
    "#         append the theta value to the retrun list\n",
    "\n",
    "#         append the left list to the retrun list\n",
    "\n",
    "#         append the right list to the retrun list\n",
    "\n",
    "#         append the class labels to the retrun list\n",
    "\n",
    "#         return the list.\n",
    "        return \n",
    "\n",
    "    def get_remainder_dict(self,dict_of_everything,index_split):\n",
    "        global fea_list\n",
    "#         init a split dict\n",
    "\n",
    "#         for each key \"ke\" in dict_of_everything:\n",
    "        for ke in dict_of_everything.keys():\n",
    "#             init a value list\n",
    "\n",
    "#             init a modified list\n",
    "\n",
    "#             get the corresponding values of the key\"ke\" \n",
    "\n",
    "#             for each value and its corresponding index of the key\"ke\" \n",
    "\n",
    "#                 if index is not in the index_split:\n",
    "\n",
    "#                     append it to the modified list and value list\n",
    "\n",
    "\n",
    "#             add this modified list to the dict\n",
    "\n",
    "#         return the splited dict and val list\n",
    "        return \n",
    "\n",
    "    #method to create decision tree\n",
    "    def create_decision_tree(self, dict_of_everything,class_val,eta_min_val):\n",
    "        global fea_list\n",
    "        #if all the class labels are same, then we are set\n",
    "        if len(set(class_val)) ==1:\n",
    "            #print(\"Leaf node for set class is\",class_val[0],len(class_val))\n",
    "            \n",
    "            return \n",
    "        #if the no class vales are less than threshold, we assign the class with max values as the class label    \n",
    "        elif len(class_val) < eta_min_val:\n",
    "            \n",
    "            \n",
    "            \n",
    "            return \n",
    "#         else:\n",
    "        else:\n",
    "#             using the best_feature to get best feature list\n",
    "\n",
    "#             store the node name, theta,left split, right split and class labes\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#             call get_remainder_dict to get left tree data\n",
    "\n",
    "#             call get_remainder_dict to get right tree data\n",
    "\n",
    "#             call create_decision_tree to get left tree based on the left tree data\n",
    "\n",
    "#             call create_decision_tree to get right tree based on therightleft tree data\n",
    "\n",
    "#             set the root node\n",
    "\n",
    "#             return root node\n",
    "            return\n",
    "            \n",
    "    #fit the decisin tree\n",
    "    def fit(self, dict_of_everything,cl_val,features,eta_min_val):\n",
    "#         set the fea_list the value of features\n",
    "        global fea_list\n",
    "    \n",
    "#         set the root node using the function create_decision_tree()\n",
    "\n",
    "        return \n",
    "\n",
    "    def classify(self,row,root):\n",
    "#         init the test dict\n",
    "\n",
    "#         add row to the dict\n",
    "        for k,j in enumerate(row):\n",
    "        \n",
    "#         set the current node to root\n",
    "\n",
    "#         while the current node is not leaf:\n",
    "        while not current_node.leaf:\n",
    "#             implement the case whether the current shoud go to the left\n",
    "\n",
    "\n",
    "#             implement the case whether the current shoud go to the right\n",
    "\n",
    "        \n",
    "#         return the calss of the current node\n",
    "        return \n",
    "        \n",
    "    #method to the labels for the test data\n",
    "    def predict(self, X, root):\n",
    "#         predict using the classify()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculating the predicited accuracy\n",
    "def accuracy_for_predicted_values(test_class_names1,l):\n",
    "#     init true and false count to 0\n",
    "\n",
    "#     for each prediction,if predict is correct then, true++ else, false++\n",
    "    for i in range(len(test_class_names1)):\n",
    "       \n",
    "           \n",
    "       \n",
    "        \n",
    "#     return the acc\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(num_arr, eta_min):\n",
    "    eta_min_val = round(eta_min*num_arr.shape[0])\n",
    "    #randomly shuffle the array so that we can divide the data into test/training\n",
    "    \n",
    "    #divide data into test labels,test features,training labels, training features\n",
    "    \n",
    "#     init cumulate acc to 0\n",
    "\n",
    "    #ten fold iteration \n",
    "    for i in range(10):\n",
    "        #build a dictionary with class labels and respective features values belonging to that class\n",
    "        \n",
    "        #instantiate decision tree instance\n",
    "        \n",
    "        # build the decision tree model.\n",
    "        \n",
    "        #predict the class labels for test features\n",
    "        \n",
    "        #calculate the accuracy for the predicted values    \n",
    "        \n",
    "#         add acc to cumulate acc\n",
    "\n",
    "        print(\"Accuracy is \",accu)\n",
    "    print(\"Accuracy across 10-cross validation for\",eta_min,\"is\",float(accu_count)/10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eta_min_list = [0.05,0.10,0.15,0.20,0.25]\n",
    "newfile = \"...\"\n",
    "num_arr = load_csv(newfile)\n",
    "for i in eta_min_list:\n",
    "    main(num_arr,i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
